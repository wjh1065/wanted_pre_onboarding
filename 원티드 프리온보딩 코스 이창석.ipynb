{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2022.02.07. 이창석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 문제1) Tokenizer 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 로드\n",
    "import re # 특수문자 제거\n",
    "from math import log # tf-idf 행렬곱 계산\n",
    "import numpy as np # tf-idf 행렬곱 계산\n",
    "import pandas as pd # 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov' : 0}\n",
    "        self.fit_checker = False\n",
    "\n",
    "    def preprocessing(self, sequences):\n",
    "        result = []\n",
    "        '''\n",
    "        문제 1-1.\n",
    "        '''\n",
    "        def clean_text(seq):\n",
    "            seq = re.sub('[-=+,#/\\?:^.@*\\\"※~ㆍ!』‘|\\(\\)\\[\\]`\\'…》\\”\\“\\’·]', ' ', seq)\n",
    "            return seq\n",
    "\n",
    "        for seq in sequences:\n",
    "            seq = seq.lower()\n",
    "            seq = clean_text(seq)\n",
    "            seq = seq.split()\n",
    "            result.append((seq))\n",
    "        return result\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        self.fit_checker = False\n",
    "        '''\n",
    "        문제 1-2.\n",
    "        '''\n",
    "        preprocessed_seqs = self.preprocessing(sequences)\n",
    "        pre_result = []\n",
    "\n",
    "        for preprocessed_seq in preprocessed_seqs:\n",
    "            pre_result += preprocessed_seq\n",
    "\n",
    "        pre_result = set(pre_result)\n",
    "\n",
    "        for num, word in enumerate(pre_result,1):\n",
    "            self.word_dict[word] = num\n",
    "\n",
    "        self.fit_checker = True\n",
    "\n",
    "    def transform(self, sequences):\n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "\n",
    "        if self.fit_checker:\n",
    "            '''\n",
    "            문제 1-3.\n",
    "            '''\n",
    "\n",
    "            for token in tokens:\n",
    "                token2index = []\n",
    "                for word in token:\n",
    "                    try:\n",
    "                        token2index.append((self.word_dict[word]))\n",
    "                    except KeyError:\n",
    "                        token2index.append((self.word_dict['oov']))\n",
    "                result.append(token2index)\n",
    "\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "\n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1) preprocessing :  [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
      "\n",
      "\n",
      "1-2) word_dict :  {'oov': 0, 'go': 1, 'to': 2, 'i': 3, 'school': 4, 'like': 5, 'pizza': 6}\n",
      "\n",
      "\n",
      "1-3) transform :  [[3, 1, 2, 4], [3, 5, 6]]\n",
      "1-3) oov_transform :  [[3, 1, 2, 4], [3, 5, 6], [3, 0, 0]]\n",
      "\n",
      "\n",
      "1-4) fit_transform :  [[3, 1, 2, 4], [3, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장\n",
    "sequences = ['I go to school.', 'I LIKE pizza!']\n",
    "oov_sequences = ['I go to school.', 'I LIKE pizza!', 'i HaTe You!~']\n",
    "tokenizer_seqs = Tokenizer()\n",
    "\n",
    "# 1-1) preprocessing()\n",
    "print('1-1) preprocessing : ', tokenizer_seqs.preprocessing(sequences))\n",
    "print('\\n')\n",
    "\n",
    "# 1-2) fit()\n",
    "tokenizer_seqs.fit(sequences)\n",
    "print('1-2) word_dict : ', tokenizer_seqs.word_dict)\n",
    "print('\\n')\n",
    "\n",
    "# 1-3) transform()\n",
    "print('1-3) transform : ', tokenizer_seqs.transform(sequences))\n",
    "print('1-3) oov_transform : ', tokenizer_seqs.transform(oov_sequences))\n",
    "print('\\n')\n",
    "\n",
    "# 1-4) fit_transform()\n",
    "print('1-4) fit_transform : ', tokenizer_seqs.fit_transform(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 문제2) TfidfVectorizer 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "        '''\n",
    "        문제 2-1.\n",
    "        '''\n",
    "        vocab = list(self.tokenizer.word_dict.keys())\n",
    "        idf_matrix = []\n",
    "        n = len(sequences)\n",
    "\n",
    "        fixed_seq = []\n",
    "        sequences = self.tokenizer.preprocessing(sequences)\n",
    "        for sequence in sequences:\n",
    "            fixed_seq.append((' '.join(sequence)))\n",
    "\n",
    "        def idf(t):\n",
    "            df = 0\n",
    "            for seq in fixed_seq:\n",
    "                df += t in seq\n",
    "            return log(n/(df+1))\n",
    "\n",
    "        for j in range(len(vocab)):\n",
    "            t = vocab[j]\n",
    "            idf_matrix.append(idf(t))\n",
    "\n",
    "        self.idf_matrix = idf_matrix\n",
    "        self.fit_checker = True\n",
    "\n",
    "    def transform(self, sequences):\n",
    "\n",
    "        vocab = list(self.tokenizer.word_dict.keys())\n",
    "        n = len(sequences)\n",
    "        tf_matrix = []\n",
    "        tfidf_matrix = []\n",
    "\n",
    "        fixed_seq = []\n",
    "        sequences = self.tokenizer.preprocessing(sequences)\n",
    "        for sequence in sequences:\n",
    "            fixed_seq.append((' '.join(sequence)))\n",
    "\n",
    "        def tf(t, s):\n",
    "            cnt = 0\n",
    "            ss = s.split( )\n",
    "            for s in ss:\n",
    "                if t == s:\n",
    "                    cnt += 1\n",
    "            return cnt\n",
    "\n",
    "        if self.fit_checker:\n",
    "            '''\n",
    "            문제 2-2.\n",
    "            '''\n",
    "            for i in range(n):\n",
    "                tf_matrix.append([])\n",
    "                s = fixed_seq[i]\n",
    "                for j in range(len(vocab)):\n",
    "                    t = vocab[j]\n",
    "                    tf_matrix[-1].append(tf(t,s))\n",
    "            tf_df_matrix = pd.DataFrame(tf_matrix, columns=vocab) # TF dataframe\n",
    "\n",
    "            self.tf_df_matrix = tf_df_matrix\n",
    "            self.tf_matrix = tf_matrix\n",
    "\n",
    "            for idx in range(n):\n",
    "                tfidf_matrix.append(np.multiply(tf_matrix[idx], self.idf_matrix))\n",
    "            tfidf_df_matrix = pd.DataFrame(tfidf_matrix, columns=vocab) # TF-IDF dataframe\n",
    "\n",
    "            self.tfidf_df_matrix = tfidf_df_matrix\n",
    "            self.tfidf_matrix = tfidf_matrix\n",
    "\n",
    "            return self.tfidf_matrix\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "\n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word dict :  {'oov': 0, 'go': 1, 'to': 2, 'i': 3, 'school': 4, 'like': 5, 'pizza': 6}\n",
      "\n",
      "\n",
      "2-1) IDF matrix :  [0.6931471805599453, 0.0, 0.0, -0.40546510810816444, 0.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "2-2)\n",
      "TF DataFrame : \n",
      "   oov  go  to  i  school  like  pizza\n",
      "0    0   1   1  1       1     0      0\n",
      "1    0   0   0  1       0     1      1\n",
      "\n",
      "\n",
      "TF matrix : \n",
      "[[0, 1, 1, 1, 1, 0, 0], [0, 0, 0, 1, 0, 1, 1]]\n",
      "\n",
      "\n",
      "TF-IDF DataFrame : \n",
      "   oov   go   to         i  school  like  pizza\n",
      "0  0.0  0.0  0.0 -0.405465     0.0   0.0    0.0\n",
      "1  0.0  0.0  0.0 -0.405465     0.0   0.0    0.0\n",
      "\n",
      "\n",
      "TF-IDF matrix : \n",
      "[array([ 0.        ,  0.        ,  0.        , -0.40546511,  0.        ,\n",
      "        0.        ,  0.        ]), array([ 0.        ,  0.        ,  0.        , -0.40546511,  0.        ,\n",
      "        0.        ,  0.        ])]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장\n",
    "sequences = ['I go to school.', 'I LIKE pizza!']\n",
    "# oov_sequences = ['I go to school.', 'I LIKE pizza!', 'i HaTe You!~']\n",
    "tokenizer_seqs = Tokenizer()\n",
    "Tfidf_seqs = TfidfVectorizer(tokenizer_seqs)\n",
    "Tfidf_seqs.fit(sequences)\n",
    "Tfidf_seqs.transform(sequences)\n",
    "\n",
    "print('word dict : ', tokenizer_seqs.word_dict)\n",
    "print('\\n')\n",
    "\n",
    "# 2-1) fit()\n",
    "print('2-1) IDF matrix : ', Tfidf_seqs.idf_matrix)\n",
    "print('\\n')\n",
    "\n",
    "# 2-2) transform()\n",
    "print('2-2)')\n",
    "print('TF DataFrame : ')\n",
    "print(Tfidf_seqs.tf_df_matrix)\n",
    "print('\\n')\n",
    "print('TF matrix : ')\n",
    "print(Tfidf_seqs.tf_matrix)\n",
    "print('\\n')\n",
    "print('TF-IDF DataFrame : ')\n",
    "print(Tfidf_seqs.tfidf_df_matrix)\n",
    "print('\\n')\n",
    "print('TF-IDF matrix : ')\n",
    "print(Tfidf_seqs.tfidf_matrix)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
