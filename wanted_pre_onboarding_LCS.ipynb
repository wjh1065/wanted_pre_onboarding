{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 원티드 프리온보딩 AI/ML코스 선발과제 이창석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 문제1) Tokenizer 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 로드\n",
    "import re # 특수문자 제거\n",
    "from math import log # tf-idf 행렬곱 계산\n",
    "import pandas as pd # 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov' : 0}\n",
    "        self.fit_checker = False\n",
    "\n",
    "    def preprocessing(self, sequences):\n",
    "        result = []\n",
    "        \n",
    "        '''\n",
    "        문제 1-1. 텍스트 전처리 [START]\n",
    "        '''\n",
    "        \n",
    "        def clean_text(seq):\n",
    "            seq = re.sub('[-=+,#/\\?:^.@*\\\"※~ㆍ!』‘|\\(\\)\\[\\]`\\'…》\\”\\“\\’·]', ' ', seq)\n",
    "            return seq\n",
    "\n",
    "        for seq in sequences:\n",
    "            seq = seq.lower() # 조건 1: 소문자 변환\n",
    "            seq = clean_text(seq) # 조건 1: 특수문자 제거\n",
    "            seq = seq.split() # 조건 2: 토큰화 white space 수행\n",
    "            result.append((seq))\n",
    "            \n",
    "        '''\n",
    "        문제 1-1. 텍스트 전처리 [END]\n",
    "        '''\n",
    "        \n",
    "        return result # 각 문장을 토큰화한 결과: nested list\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        self.fit_checker = False\n",
    "        \n",
    "        '''\n",
    "        문제 1-2. 어휘 사전 구축 [START]\n",
    "        '''\n",
    "        \n",
    "        # 조건 1: 위에서 만든 preprocessing 함수를 이용하여 각 문장 토큰화\n",
    "        preprocessed_seqs = self.preprocessing(sequences)\n",
    "        \n",
    "        pre_result = []\n",
    "        for preprocessed_seq in preprocessed_seqs:\n",
    "            pre_result += preprocessed_seq # 전처리된 각 문장 별 단어를 하나의 리스트로 합침\n",
    "\n",
    "        pre_result = set(pre_result) # set을 이용하여 문장 별 중복된 단어 합침\n",
    "\n",
    "        for num, word in enumerate(pre_result,1): # oov가 어휘 사전에 이미 value=0으로 존재하므로 '1'부터 시작\n",
    "            self.word_dict[word] = num # 조건 2: dict을 이용하여 어휘 사전 생성\n",
    "\n",
    "            \n",
    "        '''\n",
    "        문제 1-2. 어휘 사전 구축 [END]\n",
    "        '''\n",
    "        \n",
    "        self.fit_checker = True\n",
    "\n",
    "    def transform(self, sequences):\n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "\n",
    "        if self.fit_checker:\n",
    "            \n",
    "            '''\n",
    "            문제 1-3. 어휘 사전을 활용하여 입력 문장을 정수 인뎅식하는 함수 [START]\n",
    "            '''\n",
    "\n",
    "            for token in tokens:\n",
    "                token2index = []\n",
    "                for word in token:\n",
    "                    try:\n",
    "                        token2index.append((self.word_dict[word]))\n",
    "                    except KeyError: # 조건 1: 어휘 사전에 없는 단어는 'oov'의 index로 변환\n",
    "                        token2index.append((self.word_dict['oov']))\n",
    "                result.append(token2index)\n",
    "                \n",
    "            '''\n",
    "            문제 1-3. 어휘 사전을 활용하여 입력 문장을 정수 인뎅식하는 함수 [END]\n",
    "            '''\n",
    "\n",
    "            return result # 각 문장의 정수 인덱싱: nested list\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "\n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1) preprocessing :  [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
      "\n",
      "\n",
      "1-2) word_dict :  {'oov': 0, 'pizza': 1, 'i': 2, 'go': 3, 'like': 4, 'to': 5, 'school': 6}\n",
      "\n",
      "\n",
      "1-3) transform :  [[2, 3, 5, 6], [2, 4, 1]]\n",
      "1-3) oov_transform :  [[2, 3, 5, 6], [2, 4, 1], [2, 0, 0]]\n",
      "\n",
      "\n",
      "1-4) fit_transform :  [[2, 3, 5, 6], [2, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장\n",
    "sequences = ['I go to school.', 'I LIKE pizza!']\n",
    "oov_sequences = ['I go to school.', 'I LIKE pizza!', 'i HaTe You!~']\n",
    "\n",
    "tokenizer_seqs = Tokenizer()\n",
    "\n",
    "# 1-1) preprocessing()\n",
    "print('1-1) preprocessing : ', tokenizer_seqs.preprocessing(sequences))\n",
    "print('\\n')\n",
    "\n",
    "# 1-2) fit()\n",
    "tokenizer_seqs.fit(sequences)\n",
    "print('1-2) word_dict : ', tokenizer_seqs.word_dict)\n",
    "print('\\n')\n",
    "\n",
    "# 1-3) transform()\n",
    "print('1-3) transform : ', tokenizer_seqs.transform(sequences))\n",
    "print('1-3) oov_transform : ', tokenizer_seqs.transform(oov_sequences))\n",
    "print('\\n')\n",
    "\n",
    "# 1-4) fit_transform()\n",
    "print('1-4) fit_transform : ', tokenizer_seqs.fit_transform(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 문제2) TfidfVectorizer 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "        \n",
    "        '''\n",
    "        문제 2-1. 입력 문장들을 이용해 IDF 행렬을 만드는 함수 [START]\n",
    "        '''\n",
    "        \n",
    "        vocab = list(self.tokenizer.word_dict.keys()) # 문제 1에서 만든 word dict 사용함\n",
    "        n = len(sequences)\n",
    "        \n",
    "        merged_seqs = []\n",
    "        for i in tokenized: # 조건 3: 문제 1에서 만든 Tokenizer 사용\n",
    "            num2word = []\n",
    "            for id in i:\n",
    "                word_dict = vocab[id]\n",
    "                num2word.append(word_dict)\n",
    "            num2word = ' '.join(num2word)\n",
    "            merged_seqs.append(num2word)\n",
    "        \n",
    "        def idf(t): # 조건 2: idf 계산하는 함수: n은 입력된 전체 문장 개수, df는 단어 t가 포함된 문자 d의 개수\n",
    "            df = 0\n",
    "            for seq in merged_seqs:\n",
    "                df += t in seq\n",
    "            return log(n/(df+1))        \n",
    "\n",
    "        idf_matrix = []\n",
    "        for j in range(len(vocab)):\n",
    "            t = vocab[j]\n",
    "            idf_matrix.append(idf(t))\n",
    "\n",
    "        self.idf_matrix = idf_matrix # 조건 1: IDF 행렬: list\n",
    "        \n",
    "        '''\n",
    "        문제 2-1. 입력 문장들을 이용해 IDF 행렬을 만드는 함수 [END]\n",
    "        '''\n",
    "        \n",
    "        self.fit_checker = True\n",
    "\n",
    "    def transform(self, sequences):\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences)\n",
    "        \n",
    "            '''\n",
    "            문제 2-2. 입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수 [START]\n",
    "            '''\n",
    "            \n",
    "            vocab = list(self.tokenizer.word_dict.keys())\n",
    "            n = len(sequences)\n",
    "            \n",
    "            merged_seqs = []\n",
    "            for i in tokenized: # 문제 1에서 만든 Tokenizer 사용\n",
    "                num2word = []\n",
    "                for id in i:\n",
    "                    word_dict = vocab[id]\n",
    "                    num2word.append(word_dict)\n",
    "                num2word = ' '.join(num2word)\n",
    "                merged_seqs.append(num2word)\n",
    "                \n",
    "            def tf(t, s): # tf 계산하는 함수: t는 특정 단어가 등장 횟수, s는 특정 문장\n",
    "                cnt = 0\n",
    "                ss = s.split( )\n",
    "                for s in ss:\n",
    "                    if t == s:\n",
    "                        cnt += 1\n",
    "                return cnt\n",
    "            \n",
    "            tf_matrix = []\n",
    "            for i in range(n):\n",
    "                tf_matrix.append([])\n",
    "                s = merged_seqs[i]\n",
    "                for j in range(len(vocab)):\n",
    "                    t = vocab[j]\n",
    "                    tf_matrix[-1].append(tf(t,s))\n",
    "                    \n",
    "            self.tf_matrix = tf_matrix # 조건 1: TF행렬 생성\n",
    "\n",
    "            tfidf_matrix = []\n",
    "            for idx in range(n): # 조건 2: TF행렬과 문제 2-1에서 만든 IDF행렬을 곱하여 TF-IDF행렬 생성\n",
    "                product = [x*y for x,y in zip(tf_matrix[idx],self.idf_matrix)]\n",
    "                tfidf_matrix.append(product)\n",
    "            self.tfidf_matrix = tfidf_matrix\n",
    "            \n",
    "            '''\n",
    "            문제 2-2. 입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수 [END]\n",
    "            '''\n",
    "            \n",
    "            return self.tfidf_matrix # TF-IDF행렬: nested list\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "\n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word dict :  {'oov': 0, 'pizza': 1, 'i': 2, 'go': 3, 'like': 4, 'to': 5, 'school': 6}\n",
      "\n",
      "\n",
      "2-1) IDF matrix :  [0.6931471805599453, 0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "2-2)\n",
      "\n",
      "\n",
      "TF matrix : \n",
      "[[0, 0, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0]]\n",
      "\n",
      "\n",
      "TF-IDF matrix : \n",
      "[[0.0, 0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장\n",
    "sequences = ['I go to school.', 'I LIKE pizza!']\n",
    "# oov_sequences = ['I go to school.', 'I LIKE pizza!', 'i HaTe You!~']\n",
    "\n",
    "tokenizer_seqs = Tokenizer()\n",
    "\n",
    "Tfidf_seqs = TfidfVectorizer(tokenizer_seqs)\n",
    "\n",
    "Tfidf_seqs.fit(sequences)\n",
    "Tfidf_seqs.transform(sequences)\n",
    "\n",
    "print('word dict : ', tokenizer_seqs.word_dict)\n",
    "print('\\n')\n",
    "\n",
    "# 2-1) fit()\n",
    "print('2-1) IDF matrix : ', Tfidf_seqs.idf_matrix)\n",
    "print('\\n')\n",
    "\n",
    "# 2-2) transform()\n",
    "print('2-2)')\n",
    "print('\\n')\n",
    "print('TF matrix : ')\n",
    "print(Tfidf_seqs.tf_matrix)\n",
    "print('\\n')\n",
    "print('TF-IDF matrix : ')\n",
    "print(Tfidf_seqs.tfidf_matrix)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IMG_4A8190CE40A9-1](https://user-images.githubusercontent.com/67947808/153157566-0e7a354f-386b-41f2-9c47-5523ef2816cf.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
